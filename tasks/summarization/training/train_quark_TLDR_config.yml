model:
  policy_model:
    name_or_path: CarperAI/openai_summarize_tldr_sft
    input_padding_side: left
    train_generation_kwargs:
      max_length: 2048
      max_new_tokens: 256
      do_sample: False
      num_beams: 1
      temperature: 1.0
      top_k: 50
      top_p: 1.0
      num_return_sequences: 1
    valid_generation_kwargs:
      max_length: 2048
      max_new_tokens: 256
      do_sample: False
      num_beams: 1
      temperature: 1.0
      top_k: 50
      top_p: 1.0
      num_return_sequences: 1
  ref_policy:
    name_or_path: CarperAI/openai_summarize_tldr_sft
  tokenizer:
    name_or_path: EleutherAI/gpt-j-6b

reward:
  name_or_path: CarperAI/openai_summarize_tldr_sft
  load_state_dict: True
  state_dict_path: /cluster/work/sachan/sauc/summarize_from_feedback/reward_model/rm_checkpoint/pytorch_model.bin 
  batch_size: 256
  max_length: 550
  half: True

train:
  total_episodes: 468000 # 117k*4 -> total_steps = (total_episodes - 1)//batch_size + 1 = 58,500 | batch_size=8
  sample_interval: 14625 # freq. of exploration = 4 -> 58,500 / 4 = 14,625 | 14625*8 = 117k data points observed between sampling stages
  num_quantiles: 5
  kl_coef: 0.05
  entropy_coef: 0.06
  sampling_batch_size_per_card: 32
  training_batch_size_per_card: 8
  lr: 1e-5
  n_warmup_steps: 2925 # 5% of total_steps
  clip_grad: False
  max_grad_norm: 0.5
  seed: 42
  cuda_deterministic: True
  max_input_length: 1024
  unfrozen_layers_ratio: 0.3
  state_file_path: /cluster/work/sachan/sauc/nlf/quark_TLDR_5q/state.json

data:
  name_or_path: CarperAI/openai_summarize_tldr
  remote: True
  train_split_name: train
  valid_split_name: valid
  test_split_name: None

logging:
  run_name: quark_TLDR_5q
  run_id: 42
  wandb_log: True
  wandb_entity: sauc-ms-thesis
  wandb_project: nlf
  log_interval: 1000
  save_dir: /cluster/work/sachan/sauc/nlf/quark_TLDR_5q

